## 相关文件
主要文件: buf0*.*，代码行数: 6021.

内存池相关文件
文件| 代码行数|说明
---|---|---
buf0buf.h| 884|
buf0buf.ic| 641 |缓冲池对象数据结构以及相关操作函数
buf0buf.c | 1851|
buf0flu.h | 110|
buf0flu.ic | 100 | 缓冲池对于脏页刷新的管理
buf0flu.c | 869|
buf0rea.h | 98|将页从外存读取到缓冲池的相关函数
buf0rea.C | 587|
buf0lru.h | 117|
buf0lru.ic | 7 | 缓冲池中页的管理
buf0lru.C | 757|
## 概述
#### 缓冲池
- innodb可被视为基于磁盘的数据库系统（disk-base database），通常使用缓冲池技术提高数据库的整体性能
- 缓冲池是一块内存区域，通过内存来缓存频繁访问的数据，从而减少传统机械硬盘的低速对数据库性能的影响
- 数据库读取一个页，首先从磁盘读到页存放在缓冲池中，称为FIX在缓冲池；下次读到相同的页，首先判断该页是否存在缓冲池中，若在，则称缓冲池命中，直接读取；否则读取磁盘上的页
- 对页的修改，首先修改在缓冲池的页，然后以一定的频率刷新到磁盘。checkpoint机制
- 缓冲池大小直接影响数据库的整体性能。32bit操作系统最多可以将缓冲池设置为3GB，可以打开操作系统的==PAE选项==获得最大36GB的内存支持
- innodb缓冲池大小参数：innodb_buffer_pool_size
- 缓冲池数据页类型：索引页，数据页，undo页，插入缓冲（insert buffer），自适应哈希索引（adaptive hash index），锁信息
![innodb缓冲池结构](28C6F8FEDBF24B95AA68BB07FBE4A83A)
- innodb缓冲池中缓存的大部分都是B+树索引的叶子节点（data page）和非叶子节点（index page）
- 索引页、数据页、undo页、插入缓冲都可以在外部设备持久化；而自适应哈希和锁信息不能持久化
- show innodb status查看：
1. Total memory allocated 304095804; in additional pool allocated 1198592<p>
InnoDB所分配的内存总量为304095804字节,额外内存池占用1198592 字节内存
2. Buffer pool size 16384<p>
缓冲池中页的数量,这里是16384个页,总共占用256M内存(16384* 16/1024)
3. Free list length 1260<p>
缓冲池中free链表中空闲页的数量.
4. LRU length 14619<p>
缓冲池中LRU链表中页的数量.
5. Flush list length 1<p>
当前缓冲池中脏页的数量
6. Pending reads 0<p>
正在读取的页的数量,这里为0
7. Pending writes: LRU 0, flush list 0, single page 0<p>
正在进行刷新页的数量,刷新可以分为LRU、flush list、single page三种类型
8. Pages read 14619, created 0, written 40<p>
缓冲池中已经读取、创建和刷新页的次数,这里分别为14619、0、40
9. 0.00 reads/s, 0.00 creates/s, 0.11 writes/s<p>
过去一段时间内,每秒页的读取、创建和刷新的次数
10. Buffer pool hit rate 1000/ 1000<p>
缓冲池的命中率,这是监控最为关注的一个性能指标.命中率越高,数据库的性能越好,这里为100%
#### lru、free、flush链表
- 缓冲池以页（16KB）为单位申请和管理
- 缓冲池有free 链表，保存未被使用的内存页；若free链表被分配完毕，则通过LRU算法淘汰已经使用的页。
- 朴素LRU算法：最频繁使用的在LRU前端，最少使用的在LRU尾端。首先释放LRU尾端的页
- innodb LRU算法：最新的页放入midpoint位置（链表3/8处）。避免并不常用的大量页将热点数据挤出
- 自适应哈希、锁信息页用free链表申请空间，但申请完毕后不是在LRU链表中
- 缓冲池中的页被修改后，称为“脏页”，与磁盘上的页数据不一致。可通过checkpoint机制写回磁盘。需要刷回磁盘的页放入flush链表中（flush链表的页属于LRU的一部分）
#### 基本数据结构
- 缓冲池主要由两个数据结构组成,buf_pool_struct 和 buf_block_struct
- 数据结构buf_pool_struct用于内存空间的申请、状态的管理
- 数据结构buf_block_struct用于记录缓存池中每个页的信息以及状态,从而进行管理
- buf_block_struct 的定义与说明

变量 |类型 |说明
---|---|---
state| ulint| 块的状态
frame| byte* |指向页对应在缓冲池中的内存地址,实际保存页的数据
space| ulint |页的表空间id
offset| ulint |页的表空间偏移量
lock_hash_val| ulint| 页对应的hash值,由函数lock_rec_hash计算得到
lock_mutex| mutex_t* |该字段未使用
lock |rw_lock_t |保护frame变量
read_Lock| rw_lock_t| 保护frame变量
hash| buf_block_t* |hash表上用于连接具有相同hash值的块
flush_list| UT_LIST_NODE_T(buf_block_t)|flush链表,所有的脏页保存在该链表中
newest_modification |dulint| 页最新修改的lsn
oldest_modification |dulint| 页首次修改的lsn
flush_type| ulint| 表明当前页如果正在被刷新,其是从哪个链表上刷出的,有效值为:<p>BUF_FLUSH_LRU<p>BUF_FLUSH_SINGLE (实际未使用,仅定义)<p>BUF_FLUSH_LIST
free| UT_LIST_NODE_T(buf_ block_ t)|页在free链表中的位置
LRU |UT_LIST_NODE_T(buf_block_t)|页在lru链表中的位置
LRU_position| ulint| 页在lru链表中的具体位置
freed_page_clock| ulint |读取磁盘上的页时缓冲池已经刷出页的数量old ibool 该块是否在lru链表的old端
accessed| ibool |页是否被访问过
buf_fix_count| ulint| 同时对该页进行访问请求的数量
io_fix| ulint| 页正在进行的I/0操作的类型,有效值为:<p>BUF_I0_READ<p>BUF_IO_WRITE
modify_clock |dulint |具体见函数buf_page_optimistic_get_func
n_hash_helps| ulint|
n_fields |ulint|自适应哈希索引相关列,用于判断页是否需要创建对应的自适应哈希.具体说明见第10章B+树索引
n_bytes| ulint|
side |ulint|
is_hashed| ibool|
curr_n_fields| ulint|
curr_n_bytes| ulint|
curr_side| ulint|
debug_latch| rw_lock_t| debug版本下使用的latch
file_page_was_freed |ibool |页在段中是否被删除

- buf_pool_struct 定义与说明

变量 |类型 |说明
---|---|---
mutex|mutex_t|并发保护结构成员的latch
frame_mem|byte*|缓冲池申请内存空间的起始地址
frame_zero|byte*|根据16KB对齐的缓冲池内存空间的起始地址
high_end|byte*|缓冲池空间的终止地址
blocks|buf_block_t*|buf_block_struct 数组
max_size|ulint|缓冲池的最大容量,以页为单位.如缓冲池的大小为1GB,则该值为65536
curr_size|ulint|缓冲池的当前容量,以页为单位
page_hash|hash_table_t*|哈希表
n_pend_reads|ulint|当前缓冲池读取操作未完成的数量
last_printout_time|time_t|最近一次输出缓冲池信息的时间
n_pages_read|ulint|缓冲池中统计的读次数
n_pages_written|ulint|缓冲池中统计的写次数
n_pages_created|ulint|缓冲池中统计的创建次数
n_page_gets|ulint|缓冲池中统计的磁盘访问次数
n_page_gets_old| ulint|上一次缓冲池中统计的磁盘访问次数
n_pages_read_old| ulint |上一次缓冲池中统计的读次数
n_pages_written_old |ulint|上一次缓冲池中统计的写次数
n_pages_created_old| ulint|上一次缓冲池中统计的创建次数
flush_list |UT_LIST_BASE_NODE_T(buf_block_t) |flush链表
init_flush[BUF_FLUSH_LIST + 1]| ibool| 相应类型的页刷新操作是否已经开始， 三种刷新的类型分别为:<p>BUF_FLUSH_LRU<p>BUF_FLUSH_SINGLE (实际未使用,仅定义)<p>BUF_FLUSH_LIST
n_flush[BUF_FLUSH_LIST +1] | ulint|相应类型正在等待刷新页的数量
no_flush[BUF_FLUSH_LIST + 1] | OS_event_t |相应刷新的事件,用于唤醒等待脏页刷新的线程
ulint_clock |ulint |全局的clock,将页添加到LRU链表加1
freed_page_clock| ulint| 全局的clock,与数据结构buf_block_struct 中的变量freed_page_clock 相对应
LRU_flush_ended |ulint| 从LRU链表上刷新页的数量,当页从LRU链表中被替换后,该值重置为0
free| UT_LIST_BASE_NODE_T(buf_block_t)|free链表
LRU| UT_LIST_BASE_NODE_T(buf_block_t)|LRU链表
LRU_old |buf_block_t*|指向LRU链表old端起始buf_block_sruct对象,NULL,即LRU链表中的页数量,小于BUF_LRU_OLD_MIN_LEN(默认为80)
LRU_old_len| ulint| LRU链表old端中页的数量,NULL,即变量LRU_old为NULL
-缓冲池中分配的内存地址需要16KB对齐，一方面减少内存传输次数，另一方面支持 ==DIRECT_IO==（要求内存4KB对齐）
## 缓冲池的管理
#### lru算法
- innodb并没有使用朴素的LRU算法，而是midpoint insert strategy LRU
- 如果当LRU链表的长度小于BUF_ LRU_ OLD _MIN LEN(默认80)时,采用朴素的LRU算法,读到的块放入到LRU链表的第一个位置
- 若大于BUF_LRU_OLD_MIN_LEN,则将新读到的buf_ block_t块放入到buf_pool->old的后面.这部分操作由函数buf_LRU_add_block 实现。该函数有参数old, TRUE用来表示将读取到的页放入到old链表的后面,FALSE表示放入到LRU链表的首部
#### lru链表维护
- LRU尽力维护热点页在LRU链表的活跃处。若页已经在缓冲池内，被再次读取的时候会维护页在LRU中的位置，操作由函数 buf_block_make_young 完成
- 若读到的页已经处于一个比较活跃的位置，则不需要移动
- 如果热点页有被换出去的风险，需要通过函数 buf_block_make_young 将其移动到LRU链表头部，减小被替换的风险
- innodb通过buf_pool->LRU_OLD将LRU分为冷端和热端。热端 LRU_position 值越大则页热度越高，被替换的风险越小；冷端LRU_position值都相同
- 缓冲池中的页被刷新到磁盘后，会移动到LRU链表尾端，由函数buf_block_make_old实现
#### 页的分配
- 页首先从缓冲池的free链表中申请空间
- 若无可用页，向LRU链表申请，由函数buf_LRU_search_and_free_block 完成
- 尾页如果不是脏的以及没有锁则可以被替换，由函数buf_flush_read_for_replace判断
## 页的读取
#### 物理读取
- innodb启动时，缓冲池为空，所有页都在free链表。
- 数据库所有操作都要首先通过缓冲池，因此缓冲池需要通过物理读（physical read）将页读取到缓冲池中
- 读取分为同步和异步，以sync变量区分
1. 对于同步操作，操作完成后直接调用函数buf_page_io_complete释放block->lock所持有的x-latch
2. 对于异步操作，最终通过io-thread释放所有的x-latch
3. insert buffer 的bitma页以及事务系统页，读操作一定是同步的
#### 随机预读
- 随机预读是指判断某个区域内的页是否大多数已经被访问，并且这些被访问的页是否为热点数据，若满足则认为该区域的页是热点，提前预读。
- 预读根据区域内的（space，offset）顺序读取
- 预读区域根据每32（BUF_READ_AHEAD_RANDOM_AREA 宏定义，默认32）个页进行管理，即判断这32个页是否 满足随机预读条件
- 随机预读要求32个页大部分（BUF_READ_AHEAD_RANDOM_THRESHOLD宏定义，默认为9）都被访问过
- 当前9个页都要求是活跃的
- 若当前innodb压力较大，不启用随机预读，用变量buf_pool->n_pend_reads判断，若缓冲池中一半的页都在等待读取操作完成，则不会出发随机预读
- 预读是通过buf_page_read_low进行异步读取操作，转化随机读为顺序读，提高性能
#### 线性预读
- 线性预读是判断页的访问是否为顺序（如自增主键id表的扫描），如果是，则顺序读取之后或者之前的32（BUF_READ_AHEAD_LINEAR_AREA）个页
- 算法：
1. 读取一个页，若该页是某个区域的边界，并且区域内的部分页都被顺序访问过，则触发线性预读

```
low = (offset / BUF_READ_AHEAD_LINEAR_AREA)* BUF_READ_AHEAD_LINEAR_AREA;
high = (offset / BUF_READ_AHEAD_LINEAR_AREA + 1)* BUF_READ_AHEAD_LINEAR_AREA;
if ((offset != low) & (offset != high - 1)) {
/* This is not a border page of the area: return */
return(0) ;
}
```

2. 如果32个页中的12（BBUF_READ_AHEAD_LINEAR_THRESHOLD宏定义）个页
3. 线性预读要求物理上也是连续的页
4. 随机预读与线性预读的差异

差异点 | 随机预读 |线性预读
---|---|---
触发阈值|9| 12
触发判断条件 |页是否活跃| 访问是否顺序
目标页是否被读取 |否 |是
触发时间 |页被读取前 |页波读取后
#### 逻辑读取（==此节暂放==）
## 页的刷新

#### 检查点
- 事务数据库系统普遍采用了write ahead log（WAL）策略，即当提交事务时，先写重做日志，再修改页，宕机时可以通过重做日志完成数据恢复，以满足D（durablity）特性
- innodb内部有两种检查点（sharp checkpoint 和fuzzy checkpoint）
1. sharp checkpoint发生在数据库关闭时将所有的脏页刷新回到磁盘，默认方式，参数innodb_fast_shutdown=1
2. innodb内部使用fuzzy checkpoint刷新，即只刷新一部分脏页，而不是所有脏页（master thread checkpoint，async/sync flush checkpoint，FLUSH_LRU_LIST checkpoint）
- master thread checkpoint 每秒或者每10秒的速度从缓冲池中的脏页列表刷新一定比例的页回磁盘，此为异步过程
- async/sync flush checkpoint 指重做日志不可用，需要强制刷页会磁盘
- FLUSH_LRU_LIST_Checkpoint是因为InnoDB存储引擎需要保证LRU链表和free链表中需要总共有BUF_FLUSH_FREE_BLOCK_MARGIN + BUF_FLUSH_EXTRA_MARGIN (146)个可被立即使用的页.因此,在页被读取后,都会调用函数buf_flush_free_margin, 若free链表中没有足够可以使用的页,那么就需要从LRU链表中进行脏页的刷新。一旦刷新完成,页会放入到LRU链表的尾部,以供下次页的分配所使用
#### 部分写的问题
- 页的刷新可能会遇到（partial write）问题，即一个页只写了其中一部分内容，innodb使用double write功能避免
- 页刷新到磁盘首先写入到double write中，然后再写入磁盘
- double write 存在于内存的表空间中，大小为2MB，即每次最多128页的刷新
- 函数buf_flush_post_to_doublewrite用于页写入前复制到double write buffer中，满128页强制刷新到表空间的double write，然后使用buf_fluash_buffered_writes完成缓冲池页刷新到磁盘
#### 刷新的实现
- InnoDB存储引擎会刷新LRU链表中的脏页以及flush链表中的脏页,其在源码中对应的类型分别为BUF_FLUSH_LRU和BUF_FLUSH_LIST
- LRU链表中的页是根据block->LRU_position 进行排序的,old 链表位置之后的LRU_position值都相同
- flush 链表中保存的都是脏页,其是根据block->oldest_modification进行排序的,oldest_modification 值最大的页位于flush 链表的首部,当页第一次被更新时其会更新oldest_modification (就是当时重做日志的LSN),若之后还有变化则持续更新变量block->newest_modification
- 当页刷新时,其需要更新页对应的FIL_HEADER中的信息,这包括FIL_PAGE_LSN、FIL_PAGE_SPACE、FIL_PAGE_OFFSET、 CHECKSUM值,页的LSN更新的值为block->newest_modification. 对单个脏页的刷新由函数buf_flush_write_block_low完成
- 对于类型为BUF_FLUSH_LRU的刷新,还需要满足条件block->buf_fix_count等于0,也就是任何线程都不得持有页的latch。刷新时需要持有页对象的s-latch，刷新完成后通过io-thread释放s-latch
- 为了提高刷新的性能,InnoDB存储引擎还支持邻接页的刷新。其工作原理为:当刷新一个脏页时,InnoDB存储引擎会检测该页所在某个范围内的所有页,如果是脏页,并且不在LRU链表的热端,则一起进行刷新。通过AIO可以将多个1/O写入操作合并为一个1/O操作,函数buf_flush_try_neighbors用于邻接页刷新的判断与实现