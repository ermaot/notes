## linux性能优化

### 一、硬件优化

#### 1.1 网卡功能设置

使用ethtool -k 查看网卡支持的功能列表以及当前状态。

```
# ethtool -k eth0
Features for eth0:
rx-checksumming: on [fixed]
tx-checksumming: on
	tx-checksum-ipv4: off [fixed]
	tx-checksum-ip-generic: on
	tx-checksum-ipv6: off [fixed]
	tx-checksum-fcoe-crc: off [fixed]
	tx-checksum-sctp: off [fixed]
scatter-gather: on
	tx-scatter-gather: on
	tx-scatter-gather-fraglist: off [fixed]
tcp-segmentation-offload: on
	tx-tcp-segmentation: on
	tx-tcp-ecn-segmentation: on
	tx-tcp-mangleid-segmentation: off
	tx-tcp6-segmentation: on
udp-fragmentation-offload: off
generic-segmentation-offload: on
generic-receive-offload: on
large-receive-offload: off [fixed]
rx-vlan-offload: off [fixed]
tx-vlan-offload: off [fixed]
ntuple-filters: off [fixed]
receive-hashing: off [fixed]
highdma: on [fixed]
rx-vlan-filter: on [fixed]
vlan-challenged: off [fixed]
tx-lockless: off [fixed]
netns-local: off [fixed]
tx-gso-robust: on [fixed]
tx-fcoe-segmentation: off [fixed]
tx-gre-segmentation: off [fixed]
tx-gre-csum-segmentation: off [fixed]
tx-ipxip4-segmentation: off [fixed]
tx-ipxip6-segmentation: off [fixed]
tx-udp_tnl-segmentation: off [fixed]
tx-udp_tnl-csum-segmentation: off [fixed]
tx-gso-partial: off [fixed]
tx-sctp-segmentation: off [fixed]
tx-esp-segmentation: off [fixed]
tx-udp-segmentation: off [fixed]
fcoe-mtu: off [fixed]
tx-nocache-copy: off
loopback: off [fixed]
rx-fcs: off [fixed]
rx-all: off [fixed]
tx-vlan-stag-hw-insert: off [fixed]
rx-vlan-stag-hw-parse: off [fixed]
rx-vlan-stag-filter: off [fixed]
l2-fwd-offload: off [fixed]
hw-tc-offload: off [fixed]
esp-hw-offload: off [fixed]
esp-tx-csum-hw-offload: off [fixed]
rx-udp_tunnel-port-offload: off [fixed]
tls-hw-tx-offload: off [fixed]
rx-gro-hw: off [fixed]
tls-hw-record: off [fixed]

```

一般使用到

序号|功能|说明
---|---|---
1|rx-checksumming|校验接收报文的checksum。
2| tx-checksumming|计算发送报文的checksum。
3|scatter-gather|支持分散-汇聚内存方式，即发送报文的数据部分内存可以不连续，分散在多个page中。
4|tcp-segment-offload|支持TCP大报文分段。
5|udp-fragmentation-offload|支持UDP报文自动分片。
6|generic-segment-offload|当使用TSO和UFO时，一般都要打开此功能。<br>TSO和UFO都是靠网卡硬件支持，而GSO在linux中大部分是在driver层通过软件实现。<br>对于转发设备来说，个人推荐不使能GSO,开启GSO会增大转发延时。
7|rx-vlan-offload|部署在vlan网络环境内，则启用。
8|tx-vlan-offload|同上
9|receive-hashing|如果使用软件RPS/RFS功能时，再启用。

#### 1.2 网卡ring buffer配置

网卡驱动的默认ring buffer一般都不大

```
# ethtool -g eth0
Ring parameters for eth0:
Pre-set maximums:
RX:		1024
RX Mini:	0
RX Jumbo:	0
TX:		1024
Current hardware settings:
RX:		1024
RX Mini:	0
RX Jumbo:	0
TX:		1024

```

1. 在遇到burst流量时，可能导致网卡的接收ring buffer满而丢包。 
2. 在高性能大流量的服务器或者转发设备上，一般都要配置为2048甚至更高。
3. 在intel网卡驱动中，推荐发送buffer的大小设置为接收buffer的两倍

```
ethtool -G eth0 rx 2048

```

#### 1.3 中断设置
现在的网卡绝大部分都是多队列网卡，每个队列都有独立的中断。为了提高并发处理能力，我们要将不同中断分发到不同CPU核心上。通过cat  /proc/interrupts来查看硬中断的状态
```
# cat  /proc/interrupts
           CPU0       CPU1       
……

 28:         11          1   PCI-MSI-edge      virtio0-config
 29:      44698   90421577   PCI-MSI-edge      virtio0-input.0
 30:     944680   39609488   PCI-MSI-edge      virtio0-output.0

```

查看cpu亲和性

#### 1.4 网卡RSS设置 

网卡也是通过hash运算来决定把报文放在哪个接收队列中。

虽然我们无法改变hash算法，但我们可以设置hash的key，也就是通过报文的什么字段来计算，从而影响最后的结果。 

使用ethtool --show-tuple来查看指定协议

 不同网卡的RSS能力不同，支持的协议，可以设置的字段也都不同。

UDP协议的默认key，与TCP不同，只是源IP+目的IP。这样在做UDP的性能测试时，就要格外注意，使用同一台设备作为客户端，产生的UDP报文只会被分发到一个队列中，导致服务端只有一个CPU处理中断，会影响测试结果。 因此，一般我们都要通过ethtool --config-tuple来更改UDP RSS的key，使其与TCP类似

```
ethtool --show-ntuple eth0 rx-flow-hash tcp4
```

## 二、软件设置

#### 2.1 early_demux

linux收到报文后，会通过查找路由表，来判断报文是发给本机还是转发的。

如果确定是发往本机的，还要根据4层协议来查找是发给哪个socket的。

这里牵涉到了两次查找，而对于为establish状态的TCP和某些UDP来说，已经完成了“连接”，其路由可以视为“不可变”的，因此可以缓存“连接”的路由信息。

当打开了/proc/sys/net/ipv4/tcp_early_demux或udp_early_demux后，上面的两次查找可能合并为一次。

内核收到报文后，如果该4层协议使能了early_demux，就提前进行socket查找，如果找到，就直接使用socket中缓存的路由结果。该开关对于转发设备来说，无需开启 

#### 2.2 busy_poll

busy_poll最早命名为Low Latency Sockets，是为了改善内核处理报文的延时问题。

主要思想就是在做socket系统调用时，如read操作时，在指定时间内由socket层直接调用驱动层方法去poll读取报文，大概可以提升几倍的PPS处理能力。 busy_poll有两个系统层面的配置，

第一个是/proc/sys/net/core/busy_poll，其设置的是select和poll系统调用时执行busy poll的超时时间，单位为us

第二个是/proc/sys/net/core/busy_read，其设置读取操作时的busy_poll的超时时间，单位也是us。 

从测试结果上看，busy_poll的效果很明显，但其也有局限性。只有当每个网卡的接收队列有且只有一个应用会读取时，才能提高性能。如果有多个应用同时都在对一个接收队列执行busy poll时，就需要引入调度器进行裁决，白白增加消耗。

#### 2.3 tcp参数

序号|参数名|说明
---|---|---
1|tcp_abort_overflow| 控制TCP连接建立但backlog队列满时的行为。默认为0，行为是重传syn+ack，这样对端会重传ack。<br>值为1时，会直接发送RST。前者是比较温和的处理，但是不容易暴露backlog满的问题。 
2|tcp_allowed_congestion_control|显示当前系统支持的TCP流控算法 
3|tcp_congestion_control|配置当前系统使用的TCP流控算法，需要为上面显示的算法。
4|tcp_app_win|用于调整缓存大小在应用层和TCP窗口的分配。
5|tcp_dsack|是否开启Duplicate SACK。
6|tcp_fast_open|是否开启TCP Fast Open扩展。该扩展可以提高长距离通信的响应时间。
7|tcp_fin_timeout| 用于控制本端主动关闭后，等待对端FIN包的超时时间，用于避免DOS攻击，单位为秒。
8|tcp_init_cwnd|初始拥塞窗口大小。可以根据需要，设置较大的值，提高传输效率。
9|tcp_keepalive_intvl|keepalive报文的发送间隔。
10|tcp_keepalive_probes| 未收到keepalive报文回应，最大发送keepalive数量。
11|tcp_keepalive_time|TCP连接发送keepalive的空闲时间。
12|tcp_max_syn_backlog|TCP三次握手未收到client端ack的队列长度。对于服务端，需要调整为较大的值。
13|tcp_max_tw_buckets|TCP处于TIME_WAIT状态的socket数量，用于防御简单的DOS攻击。超过该数量后，socket会直接关闭。
14|tcp_sack|设置是否启用SACK，默认启用。
15|tcp_syncookies|用于防止syn flood攻击，当syn backlog队列满后，会使用syncookie对client进行验证。
16|tcp_window_scaling|设置是否启用TCP window scale扩展功能。可以通告对方更大的接收窗口，提高传输效率。默认启用

#### 2.4 socket option

序号|参数名|说明
---|---|---
1|SO_KEEPALIVE|是否使能KEEPALIVE。
2|SO_LINGER|使能LINGER选项时，当调用close或者shutdown时，如果套接字的发送缓存中有数据，不会立刻返回而是等待报文发送出去或者直到LINGER的超时时间。使能了LINGER，时间为0，会直接发送RST到对端。
3|SO_RCVBUFF|设置套接字的接收缓存大小。
4|SO_RCVTIMEO|设置接收数据的超时时间，对于服务程序来说，一般都是无阻塞，即设置为0。
5|SO_REUSEADDR|是否验证绑定的地址和端口冲突。比如已经使用ANY_ADDR绑定了某端口，则后面不能使用任何一个local地址再绑定同一个端口了。对于服务程序来说，推荐打开，避免程序重启时bind地址失败。
6|SO_REUSEPORT|允许绑定完全相同的地址和端口，更重要的是当内核收到的报文可以匹配到多个相同地址和端口的套接字时，内核会自动在这几个套接字之间做到负载均衡。

#### 2.5 其他参数

最大文件描述符数量：对于TCP服务程序来说，每个连接都要占用一个文件描述符，因此默认的最大文件描述符个数远远不够。我们需要同时增大系统和进程的最大描述符限制。前者可以使用/proc/sys/fs/file-max 可以使用/proc/sys/fs/file-max或者sysctl -n fs.file-max=xxxxxx设置。而后者可以使用ulimit -n，也可以调用setrlimit设置。

绑定CPU：服务程序的每个线程绑定到指定CPU上。可以使用taskset或者cgroup命令，将指定服务线程绑定到指定CPU或者CPU集合上。也可以调用pthread_setaffinity_np来实现。通过将指定线程绑定到CPU，一方面可以保证cache的热度（高命中），另一方面也可以做到符合业务的CPU负载分配。

#### 2.6 bypass内核

对于应用层的服务程序来说，有几个绕不开的问题，比如进出内核的数据拷贝等。于是就诞生了bypass内核的方案，如dpdk，netmap，pfring等，其中以dpdk应用最广。

相对于内核其有三个优势：1. 避免了进出内核的数据拷贝；2. 使用大页，提高了TLB的命中率；3. 默认使用poll的方式，提高了网络性能。

对于网络转发设备来说，基本上只处理二三层的报文，对协议栈要求不高。对于服务程序来说，则需要比较完备的协议栈。目前已有DPDK+mtcp、DPDK+fstack和DPDK+Nginx方案。